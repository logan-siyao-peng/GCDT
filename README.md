# GUM_Chinese

A GUM-like RST dataset for Chinese

50 documents are collected with 10 from each of the five genres.

63,000 tokens (updated on Jan 12 after gold tokenization)


## Five Genres
- academic (source: https://www.hanspub.org/)
- bio (source: https://zh.wikipedia.org/)
- interview (source: https://zh.wikinews.org/)
- news (source: https://zh.wikinews.org/)
- wikihow (source: https://zh.wikihow.com/)

## Preprocessing Steps
- XML and metadata annotations (gold)
- Paragraph and sentence splits (gold)
- Tokenization (gold) 
- Dependency parses (predicted by stanza)

## RST annotations
Guideline: see https://docs.google.com/document/d/1OmeqkDIYg5IM_pmULMzDJi__FAe7kzJmMdLtZxxd1LE/edit?usp=sharing

